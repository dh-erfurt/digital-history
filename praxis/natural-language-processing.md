# Natural Language Processing
 
*Natural Language Processing (NLP)* ist eine Teildisziplin der Informatik, in der Methoden der künstlichen Intelligenz besonders relevant sind. Sie befasst sich damit, wie Computer natürliche Sprache interpretieren, analysieren und generieren können.

Für die Digital Humanities ist *NLP* wesentlich, da es die automatisierte Verarbeitung großer Textmengen ermöglicht und so neue Perspektiven auf historische Quellen und kulturelle Artefakte eröffnet. Typische DH-Methoden, die dem Bereich zugeordnet werden, sind z. B. *Named Entity Recognition*, *Topic Modelling* oder *Sentiment Analysis*, welche im Unterkapitel [NLP-Methoden in der Geschichtswissenschaft](nlp-methoden-in-der-digital-history.md) näher vorgestellt werden.

Die *Computerlinguistik* wiederum ist eine eigenständige wissenschaftliche Disziplin an der Schnittstelle von Sprachwissenschaft und Informatik. Sie beschäftigt sich mit der computergestützten Verarbeitung natürlicher Sprache und bearbeitet insbesondere sprachtheoretische Fragestellungen mit digitalen Methoden. So stellt sie einen theoretischen und technologischen Rahmen für viele NLP-Methoden und -Verfahren zur Verfügung. Grundlegende Verfahren und Konzepte werden im Unterkapitel [Computerlinguistische Grundlagen](computerlinguistische-grundlagen.md) betrachtet.

Zudem wird Computerlinguistik gemeinhin als die Disziplin betrachtet, aus der sich die Digital Humanities entwickelt haben. Bereits in den 1950ern begannen Sprachwissenschaftler\*innen, computergestützte Methoden zur Analyse von Texten zu nutzen. Als Pionier der Digital Humanities gilt hier Roberto Busa, der mit dem *Index Thomasticus* eines der ersten großen geisteswissenschaftlichen Textkorpusprojekte in Zusammenarbeit mit IBM realisierte.