
@article{li_survey_2022,
	title = {A {Survey} on {Deep} {Learning} for {Named} {Entity} {Recognition}},
	volume = {34},
	doi = {10.1109/TKDE.2020.2981314},
	number = {1},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Li, Jing and Sun, Aixin and Han, Jianglei and Li, Chenliang},
	year = {2022},
	keywords = {Annotations, Deep learning, Encyclopedias, Natural language processing, Task analysis, Text recognition, Tools, deep learning, named entity recognition, survey},
	pages = {50--70},
}

@book{buck_wissenschaftliches_2025,
	address = {Konstanz},
	edition = {1},
	series = {Studieren, aber richtig},
	title = {Wissenschaftliches {Schreiben} mit {KI}},
	isbn = {9783825263652},
	language = {German},
	publisher = {UVK Verlag},
	author = {Buck, Isabella},
	month = mar,
	year = {2025},
	doi = {10.36198/9783838563657},
}

@misc{czmiel_generative_2024,
	title = {Generative {KI}, {LLMs} und {GPT} bei digitalen {Editionen}},
	url = {https://doi.org/10.5281/zenodo.10698210},
	publisher = {Zenodo},
	author = {Czmiel, Alexander and Dumont, Stefan and Fischer, Franz and Pollin, Christopher and Sahle, Patrick and Schaßan, Torsten and Scholger, Martina and Vogeler, Georg and Roeder, Torsten and Fritze, Christiane and Henny-Krahmer, Ulrike},
	month = feb,
	year = {2024},
	doi = {10.5281/zenodo.10698210},
}

@misc{oberbichler_working_2025,
	title = {Working {Paper}: {Implementing} {Generative} {AI} in the {Historical} {Studies}},
	url = {https://doi.org/10.5281/zenodo.14924737},
	publisher = {Zenodo},
	author = {Oberbichler, Sarah and Petz, Cindarella},
	month = feb,
	year = {2025},
	doi = {10.5281/zenodo.14924737},
}

@book{jannidis_digital_2017,
	title = {Digital {Humanities}},
	publisher = {Springer},
	author = {Jannidis, Fotis and Kohle, Hubertus and Rehbein, Malte},
	year = {2017},
}

@article{chang_survey_2024,
	title = {A {Survey} on {Evaluation} of {Large} {Language} {Models}},
	volume = {15},
	issn = {2157-6904},
	url = {https://doi.org/10.1145/3641289},
	doi = {10.1145/3641289},
	abstract = {Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, education, natural and social sciences, agent applications, and other areas. Secondly, we answer the ‘where’ and ‘how’ questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing the performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at:},
	number = {3},
	journal = {ACM Trans. Intell. Syst. Technol.},
	author = {Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and Ye, Wei and Zhang, Yue and Chang, Yi and Yu, Philip S. and Yang, Qiang and Xie, Xing},
	month = mar,
	year = {2024},
	keywords = {Large language models, benchmark, evaluation, model assessment},
}

@article{janiesch_machine_2021,
	title = {Machine learning and deep learning},
	volume = {31},
	number = {3},
	journal = {Electronic Markets},
	author = {Janiesch, Christian and Zschech, Patrick and Heinrich, Kai},
	year = {2021},
	keywords = {Analytical model building, Artificial intelligence, Artificial neural networks, Automation, Business And Economics–Computer Applications, Data analysis, Deep learning, Electronic commerce, Electronic markets, Intelligence, Intelligent systems, Machine learning, Mathematical models, Neural networks, Technology},
	pages = {685--695},
}

@article{sarkar_practical_2018,
	title = {Practical machine learning with {Python}},
	journal = {Book" Practical Machine Learning with Python},
	author = {Sarkar, Dipanjan and Bali, Raghav and Sharma, Tushar},
	year = {2018},
	pages = {25--30},
}

@article{sarker_machine_2021,
	title = {Machine learning: {Algorithms}, real-world applications and research directions},
	volume = {2},
	number = {3},
	journal = {SN computer science},
	author = {Sarker, Iqbal H},
	year = {2021},
	note = {Publisher: Springer},
	pages = {160},
}
